# Measuring Temporal Shifts in Density Estimation towards an Explainability Framework
Final Paper for Johns Hopkins University EN 625.726 Theory of Statistics II

## Abstract

This study proposes a framework for measuring temporal shifts in density estimation to enhance explainability in machine learning models. The approach leverages three complementary distance metrics—Wasserstein Distance, Kullback-Leibler (KL) Divergence, and Fisher Information Metric—to detect and characterize distribution drifts in multivariate feature spaces across time intervals. Using a credit card fraud detection dataset, the framework demonstrates how shifts in feature distributions correlate with model performance degradation, particularly when changes occur in high-importance features. The methodology offers flexibility for both global and local feature importance analysis, enabling more targeted interventions when model performance declines. Results indicate that an integrated approach combining multiple distance metrics provides a more comprehensive understanding of distribution shifts than any single metric alone, establishing a foundation for a robust model monitoring and explainability system that can adapt to temporal changes in data.

